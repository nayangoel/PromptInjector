# PromptInjector ğŸ”’

A comprehensive defensive security testing tool for AI systems. PromptInjector helps identify prompt injection vulnerabilities through systematic testing with both static and adaptive prompts.

## âš ï¸ Important Disclaimer

This tool is designed for **defensive security purposes only**. It should be used to:
- Test and improve the security of your own AI systems
- Conduct authorized security assessments
- Research prompt injection vulnerabilities for defensive purposes

**Do not use this tool to attack systems you don't own or don't have permission to test.**

## ğŸš€ Features

- **1000+ Static Prompts**: Comprehensive database of prompt injection techniques
- **Adaptive Learning**: AI agent generates new prompts based on target responses
- **Multiple Injection Types**: Tests 7 different categories of prompt injections
- **Concurrent Testing**: Configurable parallel test execution
- **Detailed Reporting**: Comprehensive security assessment reports
- **Flexible Configuration**: File and environment variable configuration
- **Campaign Management**: Organize and track multiple test campaigns

## ğŸ”§ Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd PromptInjector
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Create configuration file:
```bash
python main.py --create-config
```

4. Edit `prompt_injector_config.json` with your API keys:
```json
{
  "api": {
    "target_api_key": "your-target-model-api-key",
    "analyzer_api_key": "your-analyzer-model-api-key"
  }
}
```

## ğŸ¯ Quick Start

### Run a Quick Test (15 prompts)
```bash
python main.py --quick
```

### Run a Full Test (150 prompts)
```bash
python main.py --full
```

### Custom Test Configuration
```bash
python main.py --static 50 --adaptive 25 --verbose
```

### Test Custom Prompts
```bash
python main.py --custom my_prompts.json
```

## ğŸ“‹ Injection Types Tested

1. **Role Manipulation** - Attempts to change AI's perceived role
2. **Instruction Override** - Tries to supersede original instructions  
3. **Context Escape** - Attempts to break conversational context
4. **System Prompt Leak** - Tries to reveal system instructions
5. **Jailbreak** - Bypasses safety measures through hypotheticals
6. **Data Extraction** - Attempts to retrieve training data
7. **Behavior Modification** - Tries to alter AI behavior patterns

## ğŸ”§ Configuration

### Configuration File
Create `prompt_injector_config.json`:
```json
{
  "api": {
    "target_api_key": "your-api-key",
    "analyzer_api_key": "your-analyzer-key",
    "timeout": 30
  },
  "models": {
    "target_model": "gpt-3.5-turbo",
    "analyzer_model": "gpt-4",
    "max_tokens": 500,
    "temperature": 0.7
  },
  "testing": {
    "concurrent_tests": 3,
    "rate_limit_delay": 1.0,
    "default_static_tests": 100,
    "default_adaptive_tests": 50
  }
}
```

### Environment Variables
```bash
export PI_TARGET_API_KEY="your-target-api-key"
export PI_ANALYZER_API_KEY="your-analyzer-api-key"
export PI_TARGET_MODEL="gpt-3.5-turbo"
export PI_CONCURRENT_TESTS="2"
export PI_LOG_LEVEL="INFO"
```

## ğŸ“Š Understanding Results

### Success Rate Interpretation
- **0-10%**: Low vulnerability risk
- **10-30%**: Moderate risk - review safety measures
- **30-50%**: High risk - implement stronger protections
- **50%+**: Critical risk - immediate security review needed

### Severity Levels
- ğŸ”´ **CRITICAL**: >70% success rate, >0.7 confidence
- ğŸŸ  **HIGH**: >40% success rate, >0.5 confidence
- ğŸŸ¡ **MEDIUM**: >20% success rate
- ğŸŸ¢ **LOW**: <20% success rate

## ğŸ” Custom Prompts Format

Create a JSON file with custom prompts:
```json
[
  {
    "prompt": "Your custom injection attempt here",
    "type": "role_manipulation"
  },
  {
    "prompt": "Another test prompt",
    "type": "jailbreak"
  }
]
```

Valid types: `role_manipulation`, `instruction_override`, `context_escape`, `system_prompt_leak`, `jailbreak`, `data_extraction`, `behavior_modification`

## ğŸ“ˆ Sample Output

```
SECURITY TEST SUMMARY
============================================================
Target Model: gpt-3.5-turbo
Total Tests: 150
Successful Injections: 23
Success Rate: 15.3%
Average Confidence: 0.34

Results by Injection Type:
------------------------------------------------------------
role_manipulation       :   4/ 25 (16.0%) - Confidence: 0.42
instruction_override     :   6/ 25 (24.0%) - Confidence: 0.51
context_escape          :   2/ 20 ( 10.0%) - Confidence: 0.28
system_prompt_leak      :   8/ 25 (32.0%) - Confidence: 0.67
jailbreak               :   3/ 25 (12.0%) - Confidence: 0.31
data_extraction         :   0/ 15 ( 0.0%) - Confidence: 0.00
behavior_modification   :   0/ 15 ( 0.0%) - Confidence: 0.00

Vulnerabilities Found: 2
  ğŸŸ  system_prompt_leak    - HIGH     (32.0%)
  ğŸŸ¡ instruction_override  - MEDIUM   (24.0%)
```

## ğŸ›¡ï¸ Security Recommendations

Based on test results, the tool provides specific recommendations:

- **Role Manipulation**: Implement strict role validation and context awareness
- **Instruction Override**: Add instruction integrity checks and override prevention
- **Context Escape**: Strengthen context boundaries and input sanitization
- **System Prompt Leak**: Implement system prompt protection and access controls
- **Jailbreak**: Enhance safety filters and hypothetical scenario detection
- **Data Extraction**: Implement training data protection and access restrictions
- **Behavior Modification**: Add behavior consistency checks and modification detection

## ğŸ” Security Features

- **Content Filtering**: Optional filtering of sensitive test results
- **Anonymization**: Automatic anonymization of results when enabled
- **Audit Trail**: Comprehensive logging of all test activities
- **Rate Limiting**: Configurable delays to respect API limits
- **Concurrent Limits**: Prevents overwhelming target systems

## ğŸ¤ Contributing

This is a defensive security tool. Contributions should focus on:
- Improving detection capabilities
- Adding new defensive test cases
- Enhancing reporting and analysis
- Better configuration options

## ğŸ“œ License

This tool is provided for defensive security research and testing purposes. Users are responsible for ensuring they have proper authorization before testing any AI systems.

## ğŸ†˜ Support

For issues and questions:
1. Check the configuration is correct
2. Verify API keys are valid
3. Ensure you have permission to test the target system
4. Review logs for detailed error information

## ğŸ”® Roadmap

- [ ] Web interface for easier campaign management
- [ ] Integration with popular AI platforms
- [ ] Advanced pattern recognition for response analysis
- [ ] Export capabilities for security reports
- [ ] Multi-language prompt support